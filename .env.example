# ============================================
# ServiceNow Configuration
# ============================================
SNOW_INSTANCE=your-instance.service-now.com
SNOW_CLIENT_ID=your-client-id
SNOW_CLIENT_SECRET=your-client-secret

# Optional: Basic Auth (not recommended, use OAuth above)
# SNOW_USERNAME=your-username
# SNOW_PASSWORD=your-password

# ============================================
# LLM Provider Configuration (for OpenCode)
# ============================================

# Default LLM Provider
# Options: anthropic, openai, google, openrouter, ollama
DEFAULT_LLM_PROVIDER=anthropic

# Default Model per Provider
DEFAULT_ANTHROPIC_MODEL=claude-sonnet-4
DEFAULT_OPENAI_MODEL=gpt-4o
DEFAULT_GOOGLE_MODEL=gemini-1.5-pro
DEFAULT_OLLAMA_MODEL=llama3.1

# ============================================
# LLM Provider API Keys
# ============================================

# Anthropic (Claude)
ANTHROPIC_API_KEY=your-anthropic-api-key

# OpenAI (GPT models)
OPENAI_API_KEY=your-openai-api-key

# Google (Gemini)
GOOGLE_API_KEY=your-google-api-key

# OpenRouter (Access to many models)
OPENROUTER_API_KEY=your-openrouter-api-key

# ============================================
# Local LLM Configuration
# ============================================

# Ollama (for local/offline development)
OLLAMA_BASE_URL=http://localhost:11434

# ============================================
# OpenCode Configuration
# ============================================

# Model temperature (0.0 - 2.0)
MODEL_TEMPERATURE=1.0

# Max tokens for responses
MODEL_MAX_TOKENS=4096

# Enable auto-confirm for safe operations (use with caution)
AUTO_CONFIRM_SAFE_OPERATIONS=false

# ============================================
# Snow-Flow Configuration
# ============================================

# Log level: debug, info, warn, error
LOG_LEVEL=info

# Enable performance tracking
ENABLE_PERFORMANCE_TRACKING=true

# Enable memory system
ENABLE_MEMORY_SYSTEM=true
