# ============================================
# ServiceNow Configuration
# ============================================
SNOW_INSTANCE=your-instance.service-now.com
SNOW_CLIENT_ID=your-client-id
SNOW_CLIENT_SECRET=your-client-secret

# Optional: Basic Auth (not recommended, use OAuth above)
# SNOW_USERNAME=your-username
# SNOW_PASSWORD=your-password

# ============================================
# LLM Provider Selection (for OpenCode)
# ============================================

# Choose your default LLM provider
# Options: anthropic, openai, google, azure, cohere, mistral, groq, openrouter, ollama, perplexity
DEFAULT_LLM_PROVIDER=anthropic

# Default models per provider (uncomment the one you're using)
DEFAULT_ANTHROPIC_MODEL=claude-sonnet-4                    # Best for complex reasoning
DEFAULT_OPENAI_MODEL=gpt-4o                                # General purpose
DEFAULT_GOOGLE_MODEL=gemini-1.5-pro                        # Large context (2M tokens)
DEFAULT_AZURE_MODEL=gpt-4o                                 # Enterprise OpenAI
DEFAULT_COHERE_MODEL=command-r-plus                        # Enterprise AI
DEFAULT_MISTRAL_MODEL=mistral-large-latest                 # European AI
DEFAULT_GROQ_MODEL=llama-3.1-70b-versatile                 # Ultra-fast inference
DEFAULT_PERPLEXITY_MODEL=llama-3.1-sonar-large-128k-online # Web search enabled
DEFAULT_OLLAMA_MODEL=llama3.1                              # Local/offline

# ============================================
# üåü TIER 1: Premium Providers (Best Quality)
# ============================================

# ü§ñ Anthropic (Claude) - Best for complex ServiceNow logic
#
# ‚≠ê Option 1: Use your Claude Pro/Max subscription (RECOMMENDED if you have one)
# - ‚úÖ No API key needed! OpenCode will prompt you to log in with Anthropic
# - ‚úÖ Uses your EXISTING Claude Pro ($20/month) or Claude Max ($40/month)
# - ‚úÖ No additional costs beyond your existing subscription
# - ‚úÖ Same models, same quality, just via OpenCode instead of Claude Code
# - üîß Setup: Leave ANTHROPIC_API_KEY empty and run: opencode (then log in when prompted)
#
# Option 2: Pay-per-use API (if you don't have Claude Pro/Max)
# - Pricing: $3/$15 per 1M tokens (input/output)
# - Get API key: https://console.anthropic.com/
# - Add your key below:
ANTHROPIC_API_KEY=

# ‚ÑπÔ∏è Which should you choose?
# - Have Claude Pro/Max already? ‚Üí Use Option 1 (no extra cost!)
# - Don't have Claude Pro/Max? ‚Üí Use Option 2 OR try free local models (Ollama, LM Studio)
# - Want 100% offline/free? ‚Üí Skip Anthropic, use Ollama/LM Studio/LocalAI instead

# üß† OpenAI (GPT) - General purpose, excellent quality
# Pricing: $2.50/$10 per 1M tokens (gpt-4o)
# Get API key: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# üî∑ Azure OpenAI - Enterprise OpenAI with SLA
# Pricing: Similar to OpenAI, billed through Azure
# Setup: https://azure.microsoft.com/en-us/products/ai-services/openai-service
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# ============================================
# üöÄ TIER 2: High Performance Providers
# ============================================

# üîç Google AI (Gemini) - Massive context window (2M tokens)
# Pricing: $1.25/$5 per 1M tokens
# Get API key: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=

# üåê Perplexity - Web search enabled models
# Pricing: $1/$5 per 1M tokens (sonar models)
# Get API key: https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=

# ‚ö° Groq - Ultra-fast inference (70+ tokens/sec)
# Pricing: FREE for now (rate limited)
# Get API key: https://console.groq.com/keys
GROQ_API_KEY=

# ============================================
# üí∞ TIER 3: Cost-Effective Providers
# ============================================

# üîÄ OpenRouter - Access to 200+ models with unified API
# Pricing: Varies by model (pass-through pricing)
# Get API key: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# üá™üá∫ Mistral AI - European AI provider
# Pricing: $2/$6 per 1M tokens (mistral-large)
# Get API key: https://console.mistral.ai/api-keys
MISTRAL_API_KEY=

# üè¢ Cohere - Enterprise AI with multilingual support
# Pricing: $1/$2 per 1M tokens (command-r-plus)
# Get API key: https://dashboard.cohere.com/api-keys
COHERE_API_KEY=

# ============================================
# üè† TIER 4: Local/Offline Providers (FREE)
# ============================================

# ü¶ô Ollama - Run models locally (100% free, private, offline)
# Install: https://ollama.com
# Popular models: llama3.1, llama3.2, codellama, mistral, deepseek-coder, qwen2.5
# Setup: ollama pull llama3.1 && ollama serve
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_OLLAMA_MODEL=llama3.1

# üñ•Ô∏è LM Studio - Desktop app for local LLMs with GUI
# Download: https://lmstudio.ai
# Supports: Llama 3.2, Mistral, Phi, Gemma, DeepSeek, Qwen 2.5
# Built-in OpenAI-compatible server on port 1234
LMSTUDIO_BASE_URL=http://localhost:1234/v1
DEFAULT_LMSTUDIO_MODEL=llama-3.1-8b

# üîß LocalAI - OpenAI-compatible local server (most versatile)
# Install: https://localai.io
# Drop-in replacement for OpenAI API, supports LLMs, image gen, audio
# Docker: docker run -p 8080:8080 localai/localai
LOCALAI_BASE_URL=http://localhost:8080/v1
DEFAULT_LOCALAI_MODEL=llama3.1

# ‚ö° vLLM - High-performance inference server (2-4x faster)
# Install: pip install vllm
# Optimized for throughput and GPU efficiency
# Start: python -m vllm.entrypoints.openai.api_server --model llama3.1
VLLM_BASE_URL=http://localhost:8000/v1
DEFAULT_VLLM_MODEL=llama3.1

# üåê Text Generation WebUI (Oobabooga) - Web interface for local LLMs
# Install: https://github.com/oobabooga/text-generation-webui
# Comprehensive web UI with API support
# Enable API mode and use OpenAI-compatible endpoint
TEXT_GEN_WEBUI_BASE_URL=http://localhost:5000/v1
DEFAULT_TEXT_GEN_WEBUI_MODEL=llama3.1

# ============================================
# üéØ Provider Recommendations by Use Case
# ============================================

# üèÜ Best for ServiceNow Development:
# 1. Anthropic Claude (best reasoning)
# 2. OpenAI GPT-4o (balanced)
# 3. Groq (fastest, free)

# üíµ Most Cost-Effective:
# 1. Ollama / LM Studio / LocalAI (100% free, local)
# 2. Groq (free with limits)
# 3. OpenRouter/Mistral (cheap API)

# üåç Best for Privacy/Compliance:
# 1. Ollama / LM Studio / vLLM (fully local, 100% offline)
# 2. LocalAI (self-hosted, full control)
# 3. Azure OpenAI (enterprise SLA)

# ‚ö° Best for Performance:
# 1. vLLM (2-4x faster inference)
# 2. Groq (ultra-fast API)
# 3. Anthropic Claude (best reasoning)

# üìö Best for Large Contexts:
# 1. Google Gemini (2M tokens)
# 2. Anthropic Claude (200K tokens)
# 3. OpenAI GPT-4o (128K tokens)

# ============================================
# üîß Advanced Provider Configuration
# ============================================

# Custom OpenAI-compatible endpoints
OPENAI_BASE_URL=https://api.openai.com/v1

# Custom headers for providers (optional)
# OPENROUTER_HTTP_REFERER=https://your-app.com
# OPENROUTER_X_TITLE=Snow-Flow

# Rate limiting (requests per minute)
RATE_LIMIT_RPM=60

# ============================================
# OpenCode Configuration
# ============================================

# Model temperature (0.0 - 2.0)
MODEL_TEMPERATURE=1.0

# Max tokens for responses
MODEL_MAX_TOKENS=4096

# Enable auto-confirm for safe operations (use with caution)
AUTO_CONFIRM_SAFE_OPERATIONS=false

# ============================================
# Snow-Flow Configuration
# ============================================

# Log level: debug, info, warn, error
LOG_LEVEL=info

# Enable performance tracking
ENABLE_PERFORMANCE_TRACKING=true

# Enable memory system
ENABLE_MEMORY_SYSTEM=true
