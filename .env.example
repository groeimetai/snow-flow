# ============================================
# ServiceNow Configuration
# ============================================
SNOW_INSTANCE=your-instance.service-now.com
SNOW_CLIENT_ID=your-client-id
SNOW_CLIENT_SECRET=your-client-secret

# ============================================
# LLM Provider Configuration
# ============================================

# Choose your LLM provider
# Options: anthropic, openai, google, ollama, and more
DEFAULT_LLM_PROVIDER=anthropic

# Choose your model
DEFAULT_ANTHROPIC_MODEL=claude-sonnet-4
DEFAULT_OPENAI_MODEL=gpt-4o
DEFAULT_GOOGLE_MODEL=gemini-1.5-pro
DEFAULT_OLLAMA_MODEL=llama3.1

# ============================================
# Provider API Keys
# ============================================

# Anthropic (Claude)
# Option 1: Use Claude Pro/Max subscription (leave empty, then run: opencode auth login)
# Option 2: Use API key from https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI (GPT)
# Get API key: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Google AI (Gemini)
# Get API key: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=

# ============================================
# Local/Offline Providers (100% Free)
# ============================================

# Ollama - Run models locally
# Install: https://ollama.com
# Usage: ollama pull llama3.1 && ollama serve
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_OLLAMA_MODEL=llama3.1

# LM Studio - Desktop app with GUI
# Download: https://lmstudio.ai
OPENAI_BASE_URL=http://localhost:1234/v1
DEFAULT_LMSTUDIO_MODEL=llama-3.1-8b

# LocalAI - Self-hosted OpenAI-compatible server
# Install: https://localai.io
LOCALAI_BASE_URL=http://localhost:8080/v1
DEFAULT_LOCALAI_MODEL=llama3.1

# vLLM - High-performance inference (2-4x faster)
# Install: pip install vllm
VLLM_BASE_URL=http://localhost:8000/v1
DEFAULT_VLLM_MODEL=llama3.1

# ============================================
# Other Providers
# ============================================

# Azure OpenAI
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# Groq (Ultra-fast inference)
# Get API key: https://console.groq.com/keys
GROQ_API_KEY=

# Perplexity (Web search enabled)
# Get API key: https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=

# OpenRouter (200+ models)
# Get API key: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# Mistral AI
# Get API key: https://console.mistral.ai/api-keys
MISTRAL_API_KEY=

# Cohere
# Get API key: https://dashboard.cohere.com/api-keys
COHERE_API_KEY=

# ============================================
# Snow-Flow Configuration
# ============================================

# Log level: debug, info, warn, error
LOG_LEVEL=info

# Enable performance tracking
ENABLE_PERFORMANCE_TRACKING=true

# Enable memory system
ENABLE_MEMORY_SYSTEM=true

# ============================================
# Quick Setup Guide
# ============================================
#
# 1. Configure ServiceNow OAuth (required):
#    - SNOW_INSTANCE, SNOW_CLIENT_ID, SNOW_CLIENT_SECRET
#
# 2. Choose ONE LLM provider option:
#
#    A. Claude Pro/Max (if you have subscription):
#       DEFAULT_LLM_PROVIDER=anthropic
#       ANTHROPIC_API_KEY= (leave empty)
#
#    B. Cloud API (pay-per-use):
#       DEFAULT_LLM_PROVIDER=anthropic (or openai, google)
#       ANTHROPIC_API_KEY=your-api-key
#
#    C. Local/Free (Ollama, LM Studio, etc):
#       DEFAULT_LLM_PROVIDER=ollama
#       OLLAMA_BASE_URL=http://localhost:11434
#
# 3. Authenticate with Anthropic (if using Claude Pro/Max):
#    opencode auth login
#    → Select "Anthropic" → Select "Claude Pro/Max"
#
# 4. Authenticate with ServiceNow:
#    snow-flow auth login
#
# 5. Start developing: opencode
#
# For detailed setup instructions, see:
# - AGENTS.md (generated by snow-flow init)
# - https://github.com/groeimetai/snow-flow
