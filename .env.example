# ============================================
# ServiceNow Configuration
# ============================================
SNOW_INSTANCE=your-instance.service-now.com

# ⭐ PREFERRED: OAuth Authentication (RECOMMENDED for ALL use cases)
# Why OAuth?
#   - Modern, secure authentication
#   - Required for MCP tools (servicenow-operations, servicenow-automation, etc.)
#   - Required for snow-flow swarm command
#   - Supports token refresh (no re-authentication needed)
#   - Required for production deployments
# Setup: https://docs.servicenow.com/bundle/vancouver-platform-security/page/administer/security/task/t_SettingUpOAuth.html
SNOW_CLIENT_ID=your-client-id
SNOW_CLIENT_SECRET=your-client-secret

# ⚠️ LEGACY: Basic Authentication (Username/Password)
# Only for backwards compatibility - NOT recommended
# Limitations:
#   - Less secure than OAuth
#   - Does NOT support MCP tools
#   - Does NOT work with snow-flow swarm command
#   - Requires manual re-authentication
# Note: Leave as placeholder if using OAuth (recommended)
SNOW_USERNAME=your-username
SNOW_PASSWORD=your-password

# Authentication priority: OAuth > Basic Auth
# MCP server will try OAuth first, then fall back to basic auth if OAuth fails

# ============================================
# LLM Provider Configuration (75+ Providers!)
# ============================================

# Choose your LLM provider (set during 'snow-flow auth login')
# Popular: anthropic, openai, google, ollama, groq, mistral, cohere, deepseek
# Local: ollama, lmstudio, jan, localai, vllm
# Enterprise: azure-openai, aws-bedrock, gcp-vertexai
# Full list: https://models.dev (75+ providers via OpenCode!)
DEFAULT_LLM_PROVIDER=

# Default model for snow-flow swarm command (set during 'snow-flow auth login')
# This prevents context window issues (e.g., grok-3-mini is too small)
# Examples: claude-sonnet-4, gpt-4o, gemini-2.0-flash-exp, llama3.3:70b
DEFAULT_MODEL=

# Provider-specific model defaults (for reference)
DEFAULT_ANTHROPIC_MODEL=claude-sonnet-4.5
DEFAULT_OPENAI_MODEL=gpt-5
DEFAULT_GOOGLE_MODEL=gemini-2.5-flash
DEFAULT_OLLAMA_MODEL=llama3.1

# ============================================
# Provider API Keys
# ============================================

# Anthropic (Claude)
# Option 1: Use Claude Pro/Max subscription (leave empty, then run: opencode auth login)
# Option 2: Use API key from https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI (GPT)
# Get API key: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Google AI (Gemini)
# Get API key: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=

# ============================================
# Local/Offline Providers (100% Free)
# ============================================

# Ollama - Run models locally
# Install: https://ollama.com
# Usage: ollama pull llama3.1 && ollama serve
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_OLLAMA_MODEL=llama3.3

# LM Studio - Desktop app with GUI
# Download: https://lmstudio.ai
OPENAI_BASE_URL=http://localhost:1234/v1
DEFAULT_LMSTUDIO_MODEL=llama-3.3-8b

# LocalAI - Self-hosted OpenAI-compatible server
# Install: https://localai.io
LOCALAI_BASE_URL=http://localhost:8080/v1
DEFAULT_LOCALAI_MODEL=llama3.3

# vLLM - High-performance inference (2-4x faster)
# Install: pip install vllm
VLLM_BASE_URL=http://localhost:8000/v1
DEFAULT_VLLM_MODEL=llama3.3

# ============================================
# Other Cloud Providers (60+ more available!)
# ============================================

# Groq (Ultra-fast inference - 800+ tokens/sec)
# Get API key: https://console.groq.com/keys
GROQ_API_KEY=

# DeepSeek (Specialized for coding)
# Get API key: https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=

# Mistral AI (European alternative)
# Get API key: https://console.mistral.ai/api-keys
MISTRAL_API_KEY=

# Cohere (Command R+ models)
# Get API key: https://dashboard.cohere.com/api-keys
COHERE_API_KEY=

# Perplexity (Web search enabled)
# Get API key: https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=

# OpenRouter (200+ models aggregator)
# Get API key: https://openrouter.ai/keys
OPENROUTER_API_KEY=

# Together AI (Open source models)
# Get API key: https://api.together.xyz/settings/api-keys
TOGETHER_API_KEY=

# Replicate (Run any model)
# Get API key: https://replicate.com/account/api-tokens
REPLICATE_API_KEY=

# ============================================
# Enterprise Providers
# ============================================

# Azure OpenAI (Enterprise)
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o

# AWS Bedrock (Enterprise)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1

# GCP Vertex AI (Enterprise)
GCP_PROJECT_ID=
GCP_LOCATION=us-central1
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json

# ============================================
# Snow-Flow Configuration
# ============================================

# Log level: debug, info, warn, error
LOG_LEVEL=info

# Enable performance tracking
ENABLE_PERFORMANCE_TRACKING=true

# Enable memory system
ENABLE_MEMORY_SYSTEM=true

# ============================================
# Quick Setup Guide (75+ LLM Providers Supported!)
# ============================================
#
# 1. Configure ServiceNow OAuth (REQUIRED - recommended for all use cases):
#    - SNOW_INSTANCE=your-instance.service-now.com
#    - SNOW_CLIENT_ID=your-client-id
#    - SNOW_CLIENT_SECRET=your-client-secret
#    - Note: Leave SNOW_USERNAME and SNOW_PASSWORD as placeholders
#    - Setup OAuth: https://docs.servicenow.com/oauth
#
# 2. Choose ONE LLM provider option from 75+ providers:
#
#    A. Claude Pro/Max (if you have subscription):
#       DEFAULT_LLM_PROVIDER=anthropic
#       ANTHROPIC_API_KEY= (leave empty)
#
#    B. Cloud API (pay-per-use):
#       DEFAULT_LLM_PROVIDER=anthropic (or openai, google, groq, mistral, deepseek, etc.)
#       ANTHROPIC_API_KEY=your-api-key
#
#    C. Local/Free (Ollama, LM Studio, Jan, etc):
#       DEFAULT_LLM_PROVIDER=ollama
#       OLLAMA_BASE_URL=http://localhost:11434
#
#    D. Enterprise (Azure, AWS, GCP):
#       DEFAULT_LLM_PROVIDER=azure-openai (or aws-bedrock, gcp-vertexai)
#       [Configure enterprise credentials above]
#
# 3. Authenticate:
#    snow-flow auth login
#    (Automatically handles both LLM provider AND ServiceNow!)
#
# 4. Start developing:
#    snow-flow swarm "create incident dashboard"
#    (or launch OpenCode: opencode)
#
# For detailed setup and full provider list, see:
# - README.md (75+ provider options explained)
# - https://models.dev (complete provider catalog)
# - https://github.com/groeimetai/snow-flow
